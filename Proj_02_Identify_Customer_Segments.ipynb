{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb8938b",
   "metadata": {},
   "source": [
    "## Proj_02 : Identify Customer Segments\n",
    "- Source : UdaCity, Machine Learning Intro, Unsupervided Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5d36c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, I will apply unsupervised learning techniques to ****identify segments of the population that form the core customer base**** for a mail-order sales company in Germany.<br>\n",
    "These segments can then be used to direct marketing campaings audiences that will have the highest expected rate of returns.<br>\n",
    "The data that you will use has been provided by our partners at `Bertelsmann Arvato Analytics`, and represents a real-life data science task.\n",
    "\n",
    "## Goal\n",
    "\n",
    "****Each row**** of the demographics files represents ****a single person**** , but also includes information outside of individuals, including information about their household, building, and neighborhood. <br>\n",
    "\n",
    "I will use this information to ****cluster the general population into groups**** with similar demographic properties. \n",
    "Then, I will see how the people in the customers dataset fit into those created clusters.<br> \n",
    "\n",
    "The hope here is that certain clusters are over-represented in the customers data, as compared to the general population; those over-represented clusters will be assumed to be part of the core userbase. This information can then be used for further applications, such as targeting for a marketing campaign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bbc2e5",
   "metadata": {},
   "source": [
    "## Overview of this project\n",
    "<img src=\"./Data/Proj_02/UC_Proj_02.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae88c0",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_0\"></a>\n",
    "## Contents\n",
    "1. [Exploring the Dataset](#mokuji_1)<br>\n",
    "  1-1. [Data_Dictionary.md](#mokuji_1_1)<br>\n",
    "  1-2. [AZDIAS_Feature_Summary.csv](#mokuji_1_2)<br>\n",
    "  1-3. [UdaCity_AZDIAS_Subset.csv](#mokuji_1_3)<br>\n",
    "  1-4. [Udacity_CUSTOMERS_Subset.csv](#mokuji_1_4)<br>\n",
    "  <br>\n",
    "2. [Preprocessing the Dataset](#mokuji_2)<br>\n",
    "  2-1. [Convert Missing or Unknown Values to NANs](#mokuji_2_1)<br>\n",
    "  2-2. [Discussions about Nan in Each Column](#mokuji_2_2)<br>\n",
    "  2-3. [Discussions about Nan in Each Row](#mokuji_2_3)<br>\n",
    "  2-4. [Discussions about Data Types](#mokuji_2_4)<br>\n",
    "  2-5. [Encoding](#mokuji_2_5)<br>\n",
    "      2-5-1. [Encoding : `categorical`](#mokuji_2_5_1)<br>\n",
    "      2-5-2. [Encoding : `mixed`](#mokuji_2_5_2)<br>\n",
    "  2-6. [Scaling](#mokuji_2_6)<br>\n",
    "  2-7. [Cleaning Function](#mokuji_2_7)<br>\n",
    "  <br>\n",
    "3. [Dimensionality Reduction](#mokuji_3)<br>\n",
    "  3-1. [PCA, First Trial](#mokuji_3_1)<br>\n",
    "  3-2. [Check Out the PCA Results](#mokuji_3_2)<br>\n",
    "  3-3. [PCA, Re-Fit](#mokuji_3_3)<br>\n",
    "  <br>\n",
    "4. [Clusterling](#mokuji_4)<br>\n",
    "  4-1. [Apply Clustering to General Population](#mokuji_4_1)<br>\n",
    "  4-2. [Apply All Steps to the Customer Data](#mokuji_4_2)<br>\n",
    "  4-3. [Compare Customer Data to Demographics Data](#mokuji_4_3)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef119a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# show all observation-related columns/rows\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e41f8",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_1\"></a>\n",
    "## [1. Exploring the Dataset](#mokuji_0)\n",
    "\n",
    "There are four files in the directory \"./Data/Proj_02\"\n",
    "\n",
    "1. Data_Dictionary.md\n",
    "    - This file includes detail explanation of all features.\n",
    "    - From this file, I found that main dataset of this project are `UdaCity_AZDIAS_Subset.csv` and `UdaCity_CUSTOMERS_Subset.csv`.\n",
    "2. AZDIAS_Features_Summary.csv\n",
    "    - Summary of feature attributes for demographics data. \n",
    "    - 85 features(rows) x 4 columns.\n",
    "3. UdaCity_AZDIAS_Subset.csv\n",
    "    - Demographics data for the general population of Germany.\n",
    "    - 891211 persons(rows) x 85 features(columns).\n",
    "4. UdaCity_CUSTOMERS_Subset.csv\n",
    "    - Demographics data for customers of a mail-order company.\n",
    "    - 191652 persons (rows) x 85 features (columns).\n",
    "\n",
    "The fiture below is the relationship of data.\n",
    "<img src=\"./Data/Proj_02/ER_01.PNG\" width=\"450\"/>\n",
    "\n",
    "I will explore these dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a1556",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_1_1\"></a>\n",
    "### [1-1. Data_Dictionary.md](#mokuji_0)\n",
    "\n",
    "Detailed information file about the features in the provided datasets.<br>\n",
    "In `Data_Disctionary.md`, there are explanations for data.<br>\n",
    "I show the contents example in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import my libarary\n",
    "from Library.my_common_tool import display_text_file\n",
    "display_text_file(\"./Data/Proj_02/Data_Dictionary.md\", line_start=33, line_end=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc8517",
   "metadata": {},
   "source": [
    "To get the relationship about `attribute` and `explanation` of `Data_Disctionary.md`, I convert `Data_Disctionary.md` into `Data_Disctionary.csv` and create the variable `my_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import my libarary\n",
    "from Library.Proj_02.my_data_process import convert_dict_md_to_csv\n",
    "#\n",
    "# Convert .md to .csv\n",
    "file_path_dict = \"./Data/Proj_02/Data_Dictionary.md\"\n",
    "convert_dict_md_to_csv(\"./Data/Proj_02/Data_Dictionary.md\")\n",
    "#\n",
    "my_dict = pd.read_csv(\"./Data/Proj_02/Data_Dictionary.csv\", sep=\"\\t\")\n",
    "display(\"my_dict size is {}\".format(my_dict.shape))\n",
    "display(my_dict.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a79f42",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_1_2\"></a>\n",
    "### [1-2. AZDIAS_Feature_Summary.csv](#mokuji_0)\n",
    "\n",
    "Summary of feature attributes for demographics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da77745",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_info = pd.read_csv(\"./Data/Proj_02/AZDIAS_Feature_Summary.csv\", sep=\";\")\n",
    "display(feat_info.shape)\n",
    "display(feat_info.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Nans\n",
    "print(\"Number of Nans\")\n",
    "print(\"--------------------------\")\n",
    "feat_info.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f66554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import my library\n",
    "from Library.my_common_tool import display_unique_values\n",
    "#\n",
    "print(\"Unique Values\")\n",
    "col_names_of_string_value = [\"attribute\", \"information_level\", \"type\", \"missing_or_unknown\"]\n",
    "display_unique_values(feat_info, col_names_of_string_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ff8f2",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_1_3\"></a>\n",
    "### [1-3. UdaCity_AZDIAS_Subset.csv](#mokuji_0)\n",
    "\n",
    "Demographics data for the general population of Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = pd.read_csv(\"./Data/Proj_02/Udacity_AZDIAS_Subset.csv\", sep=\";\")\n",
    "display(azdias.shape)\n",
    "display(azdias.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a6400",
   "metadata": {},
   "source": [
    "I seek how may Nan is there in each `column` of `azdias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Nans\n",
    "from Library.my_common_tool import plot_nan_ratio_each_column, plot_nan_ratio_each_row\n",
    "title = \"Percentage of Nan in each column : azdias\"\n",
    "plot_nan_ratio_each_column(azdias, title,figsize=(16,4),fontsize=14,bRemoveZeroNanData=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087c9e1",
   "metadata": {},
   "source": [
    "In the cell below, there are some data which have big value.<br>\n",
    "So, I think that normalization will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac545ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import my library\n",
    "from Library.my_common_tool import plot_summary\n",
    "\n",
    "# Show Data Summary\n",
    "title = \"Summary : azdias\"\n",
    "fig = plot_summary(azdias, title,figsize=(18,4),fontsize=14)\n",
    "plt.figure(fig)\n",
    "plt.ylim((0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c97fc4",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_1_4\"></a>\n",
    "### [1-4. Udacity_CUSTOMERS_Subset.csv](#mokuji_0)\n",
    "\n",
    "Demographics data for customers of a mail-order company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv(\"./Data/Proj_02/Udacity_CUSTOMERS_Subset.csv\", sep=\";\")\n",
    "display(customers.shape)\n",
    "display(customers.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb480aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Nans\n",
    "title = \"Percentage of Nan in each column : customers\"\n",
    "plot_nan_ratio_each_column(customers, title,figsize=(18,4),fontsize=14,bRemoveZeroNanData=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Data Summary\n",
    "title = \"Summary : customers\"\n",
    "fig = plot_summary(customers, title,figsize=(18,4),fontsize=14)\n",
    "plt.figure(fig)\n",
    "plt.ylim((0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53d601",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2\"></a>\n",
    "## [2. Preprocessing the Dataset](#mokuji_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173354f",
   "metadata": {},
   "source": [
    "In this section, I will process following below:\n",
    "1. Convert Missing or Unknown Values to NANs\n",
    "    - `missing_or_unknow` of `feat_info` are used as a flag to find data which have no meaning.\n",
    "    - `azdias` will be modified according to the flag(`missing_or_unknow`).\n",
    "2. Assess Nan in Each Column\n",
    "    - I want to know how to treat nan. So, I visualize the percentage of nan.\n",
    "    - I will show features which do not have nan.\n",
    "    - I will drop features which have too much nan.\n",
    "3. Encoding non-numecial data\n",
    "    - Features of `azdias` whose `type` are `categorical` or `mixed` are non-numerical. So, I preprocess them with one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe5a4e",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_1\"></a>\n",
    "### [2-1. Convert Missing or Unknown Values to NANs](#mokuji_0)\n",
    "\n",
    "From the cell above, unique values of `missing_or_unkown` of `feat_info` are <br>\n",
    "[-1,0], [-1,0,9], [0], [-1], [], [-1,9], [-1,X], [XX], [-1,XX].<br>\n",
    "The types of these alues are string. So, I will process `azdias` folloing below:\n",
    "1. I get a integer `value A` from strings (`[-1,0], [-1,0,9], [0], [-1], [], [-1,9], [-1,X], [XX], [-1,XX]`)\n",
    "2. `value A` is `missing` or `unknow`, so `value A` has no meaning. I will replace `value A` in `azdias` of `np.nan`.\n",
    "\n",
    "In the next cell, I implement the function to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_miss_to_nan(feat_info, df):\n",
    "    for id_feat in range(feat_info.shape[0]):\n",
    "        feature_name = feat_info[\"attribute\"][id_feat]\n",
    "        miss_string  = feat_info[\"missing_or_unknown\"][id_feat]\n",
    "        \n",
    "        if feature_name not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # replace unnecessary string\n",
    "        miss_string = miss_string.replace(\"[\", \"\")\n",
    "        miss_string = miss_string.replace(\"]\", \"\")\n",
    "        miss_string = miss_string.replace(\" \", \"\")\n",
    "        \n",
    "        # split string\n",
    "        miss_string_list = miss_string.split(\",\")\n",
    "        \n",
    "        # replace \"miss value\" to \"np.nan\"\n",
    "        for tmp_str in miss_string_list:\n",
    "            if(tmp_str == \"X\") or (tmp_str == \"XX\"):\n",
    "                df[feature_name] = df[feature_name].replace(tmp_str, np.nan)\n",
    "                #print(\"{} / Find X or XX : {}\".format(feature_name, tmp_str))\n",
    "            elif (len(tmp_str)==0):\n",
    "                df[feature_name] = df[feature_name].replace(tmp_value, np.nan)\n",
    "                #print(\"{} / Find Empty\".format(feature_name))\n",
    "            else:\n",
    "                tmp_value = int(tmp_str)\n",
    "                df[feature_name] = df[feature_name].replace(tmp_value, np.nan)\n",
    "                #print(\"{} / Find {}\".format(feature_name,tmp_value))\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd502c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique_values(df):\n",
    "    for column_name in df.columns:\n",
    "        print(\"{} : {}\".format(column_name, df[column_name].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NAN : Before converting missing to nan\")\n",
    "azdias.isnull().sum(axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ef069",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5161fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver missing to nan\n",
    "azdias = conv_miss_to_nan(feat_info, azdias)\n",
    "\n",
    "print(\"Number of NAN : After converting missing to nan\")\n",
    "azdias.isnull().sum(axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_unique_values(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734c04f",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_2\"></a>\n",
    "### [2-2. Discussions about Nan in Each Column](#mokuji_0)\n",
    "\n",
    "I will visualize how may Nan is there in each `column` of `azdias`.\n",
    "From the graph below, I see that \n",
    "  - From Fig-2-1\n",
    "    - Top 6 features(`TITEL_KZ`, `AGER_TYP`, `KK_KUNDENTYP`, `KBA05_BAUMAX`, `GEBURTSJAHR`,`ALTER_HH`) have large percentage of nan. Other data have only less than 20% of nan.\n",
    "    - I will drop 6 features.\n",
    "  - From Fig-2-2\n",
    "    - Features which have no nan are belonged to the `information_level` **person**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ba496",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fig-2-1.\")\n",
    "title = \"Percentage of Nan in each column : azdias after processing missing_or_unknown\"\n",
    "fig, ser_nan_count_azdias = plot_nan_ratio_each_column(azdias, title,figsize=(16,4),fontsize=14,bRemoveZeroNanData=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fig-2-2.\")\n",
    "ser_nan_count_azdias.rename(\"percent_of_nan\")\n",
    "df_nan_count_azdias = pd.DataFrame(ser_nan_count_azdias, columns=[\"percent_of_nan\"])\n",
    "df_nan_count_azdias[\"attribute\"] = df_nan_count_azdias.index\n",
    "feat_info2 = pd.merge(feat_info, df_nan_count_azdias, on=\"attribute\", how=\"left\")\n",
    "feat_info2.sort_values(by=[\"percent_of_nan\"], ascending=False, inplace=True)\n",
    "display(feat_info2.sort_values(\"type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1de47",
   "metadata": {},
   "source": [
    "### I will drop features( = columns ) which have large ratio of nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ef50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols_with_nan(df, thresh_percentage):\n",
    "    # get the percentage of nan\n",
    "    ser_num_of_nan = df.isnull().sum(axis=0)\n",
    "    ser_num_of_nan = ser_num_of_nan / df.shape[0] * 100\n",
    "    # \n",
    "    col_names_necessary = ser_num_of_nan[ser_num_of_nan <= thresh_percentage].index\n",
    "    col_names_to_drop   = list(ser_num_of_nan[ser_num_of_nan >  thresh_percentage].index)\n",
    "    #\n",
    "    df_to_drop   = df[col_names_to_drop]\n",
    "    df_necessary = df[col_names_necessary]\n",
    "    #\n",
    "    print(\"col_names_to_drop = \\n{}\\n\".format(col_names_to_drop))\n",
    "    #\n",
    "    return df_necessary, df_to_drop, col_names_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072df360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data size before remove columns with nan = \", azdias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_mod, _, col_names_to_drop = remove_cols_with_nan(azdias, thresh_percentage=30.0)\n",
    "print(\"Data size after remove columns with nan = \", azdias_mod.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8067b7",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_3\"></a>\n",
    "### [2-3. Discussions about Nan in Each Row](#mokuji_0)\n",
    "\n",
    "Now I will perform a similar assessment for the rows of the dataset. <br>\n",
    "How much data is missing in each row?\n",
    "\n",
    "First I overview of percentage of Nan in each row.<br>\n",
    "From the graph below, about **70%** of rows have no Nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fig-2-4.\")\n",
    "title = \"Percentage of Nan in each row : azdias after dropping of nan\"\n",
    "res, ser_nan_count_azdias_each_row = plot_nan_ratio_each_row(azdias_mod, title,figsize=(16,4),fontsize=14,bRemoveZeroNanData=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d483e",
   "metadata": {},
   "source": [
    "If the distributions of non-missing features look similar between the data with missing values, I could argue that simply dropping those points from the analysis won't present a major issue.\n",
    "\n",
    "On the other hand, if the data with missing values looks very different from the data with non-missing values, then I should make a note on those data as special.\n",
    "\n",
    "So I separate `azdias_mod` to two dataset.\n",
    "  - `azdias_wo_nan` : This dataset has no nan.\n",
    "  - `azdias_w_nan`  : This dataset has nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_nan(df, thresh_percentage):\n",
    "    # get the percentage of nan\n",
    "    ser_num_of_nan = df.isnull().sum(axis=1)\n",
    "    ser_num_of_nan = ser_num_of_nan / df.shape[1] * 100\n",
    "    # \n",
    "    row_ids_wo_nan = ser_num_of_nan[ser_num_of_nan <= thresh_percentage].index\n",
    "    row_ids_w_nan  = ser_num_of_nan[ser_num_of_nan >  thresh_percentage].index\n",
    "    #\n",
    "    df_wo_nan = df.iloc[row_ids_wo_nan, :]\n",
    "    df_w_nan  = df.iloc[row_ids_w_nan,  :]\n",
    "    #\n",
    "    return df_wo_nan, df_w_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Before Drop: \", azdias_mod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_wo_nan, azdias_w_nan = remove_rows_with_nan(azdias_mod, thresh_percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eba23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of After Drop: \", azdias_wo_nan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a4c40",
   "metadata": {},
   "source": [
    "I will display the comparison graphs of `azdias_wo_nan` and `azdias_w_nan`. <br>\n",
    "From the graphs below, almost distribution look similar.<br>\n",
    "So, I think I can remove rows with nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ac121",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,2)\n",
    "plt.rcParams[\"font.size\"]      = 12\n",
    "    \n",
    "col_names = azdias_mod.columns\n",
    "\n",
    "for i, col_name in enumerate(col_names):\n",
    "    fig = plt.figure()\n",
    "    #\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    sns.countplot(x=azdias_mod[col_name])\n",
    "    if i==0:\n",
    "        ax1.set_title(\"azdias_mod without Nan\")\n",
    "    #\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    sns.countplot(x=azdias_mod[col_name])\n",
    "    if i==0:\n",
    "        ax2.set_title(\"azdias_mod with Nan\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450176e6",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_4\"></a>\n",
    "### [2-4. Discussions about Data Types](#mokuji_0)\n",
    "\n",
    "Before proceeding to the encoding step, I show data types in the dataset.<br>\n",
    "I show unique values for each type in the cell below.<br>\n",
    "From the results of below cell, I can see 5 data types.\n",
    " 1. `numeric`  ---> The size of value has meanings. So, I can use this without modification.\n",
    " 2. `ordinal`  ---> The size of value has meanings. So, I can use this without modification.\n",
    " 3. `interval` ---> The size of value has meanings. So, I can use this without modification.\n",
    " 4. `categorical`  ---> The size of value has **no meanings**. So I need to modify this.\n",
    "    - For `categorical` data, I will apply one-hot encoding.\n",
    " 5. `mixed`  ---> The size of value has **no meanings**. So I need to modify this.\n",
    "    - For `mixed` data, I will explain how to treat them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique_variables(df, data_type):\n",
    "    feature_names_all = feat_info[feat_info.type == data_type][\"attribute\"].values\n",
    "    feat_names_in_df  = df.columns\n",
    "    #\n",
    "    print(\"=====================\")\n",
    "    print(\"Type = \", data_type)\n",
    "    print(\"=====================\")\n",
    "    #\n",
    "    for tmp_name in feature_names_all:\n",
    "        if (tmp_name in feat_names_in_df):\n",
    "            unique_values = df[tmp_name].unique()\n",
    "            print(\"column = \", tmp_name)\n",
    "            print(\"       \", unique_values)\n",
    "    #\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in [\"numeric\", \"ordinal\", \"interval\", \"categorical\", \"mixed\"]:\n",
    "    display_unique_variables(azdias_wo_nan, data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04859c35",
   "metadata": {},
   "source": [
    "Careful observation of `mixed` revealed the following:\n",
    "- **PRAEGENDE_JUGENDJAHRE** combines information on two dimensions:\n",
    "  1. generation by decade\n",
    "    - e.g. = 40s, 50s, 60s, 70s, 80s, 90s\n",
    "  2. movement\n",
    "    - e.g. = Mainstream, Avantgarde\n",
    "    \n",
    "- I will create two new variables to capture the other two dimensions.\n",
    "  1. **PRAEGENDE_JUGENDJAHRE_GEN** = continuous numerical data\n",
    "  2. **PRAEGENDE_JUGENDJAHRE_MOV** = binary numerical data\n",
    "  \n",
    "---\n",
    "- **CAMEO_INTL_2015** combines information on two dimensions:\n",
    "  1. wealth : first digit shows wealth level\n",
    "  2. lifestage : second digit shows lifestage level\n",
    "    \n",
    "- I will create two new variables to capture the other two dimensions.\n",
    "  1. **CAMEO_INTL_2015_WEALTH** = continuous numerical data from 1 to 5\n",
    "  2. **CAMEO_INTL_2015_LIFESTAGE** = continuous numerical data from 1 to 5\n",
    "  \n",
    "---\n",
    "- Other features in `mixed`\n",
    "  - I will drop these features because I know these features are unnecessary from this dataset comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db807a6",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_5\"></a>\n",
    "### [2-5. Encoding](#mokuji_0)\n",
    "\n",
    "In the cell above, I investigated data distribution of `azdias` and applied three function to `azdias`.\n",
    "  1. `conv_miss_to_nan(feat_info, azdias)` : This converts `missing_or_unknown` to `nan`.\n",
    "  2. `remove_cols_with_nan(azdias, thresh_percentage=20.0)` : This removes columns which have nans over threshold.\n",
    "  3. `remove_rows_with_nan(azdias, thresh_percentage=0.1)` : This removes rows which have nans over threshold.\n",
    "\n",
    "In this section, I will apply some encoding processes to data with the policy which was explained in the cells above.\n",
    "\n",
    "#### Encoding policy for `categorical`\n",
    "- Policy-1. two level numerical data => 0 or 1\n",
    "- Policy-2. two level character data => 0 or 1\n",
    "- Policy-3. multi level numerical data => one-hot-encoding\n",
    "- Policy-4. multi level character data => one-hot-encoding\n",
    "\n",
    "\n",
    "- I will create two functions.\n",
    "  - Func-1. `get_categorical_featrues_to_encode`\n",
    "  - Func-2. `edencode_categorical(azdias)`\n",
    "\n",
    "\n",
    "- To reduce data size, I will transform the data type of `categorical` to `uint8`.\n",
    " \n",
    "    \n",
    "#### Encoding policy for `mixed`\n",
    "- I explained in the previous cell.\n",
    "- I will create a function `encode_mix(azdias)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ea09a",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_5_1\"></a>\n",
    "### [2-5-1. Encoding : `categorical`](#mokuji_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f3ba2",
   "metadata": {},
   "source": [
    "### Get feature names of `azdias_wo_nan` whose type are `categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c041ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_all = list(feat_info[feat_info[\"type\"]==\"categorical\"].attribute)\n",
    "categorical_features_now = set(list(azdias_wo_nan.columns)) & set(categorical_features_all)\n",
    "display(\"categorical_features_now = \", categorical_features_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_featrues_to_encode(feat_info, df, categorical_col_names, flag_print=False):\n",
    "    two_level_num = []\n",
    "    two_level_str = []\n",
    "    mul_level_num = []\n",
    "    mul_level_str = []\n",
    "    str_apo  = \"\\'\"\n",
    "    type_str = \"<class \" + str_apo + \"str\" + str_apo + \">\"\n",
    "    #\n",
    "    all_col_names    = list(df.columns)\n",
    "    target_col_names = set(categorical_col_names) & set(all_col_names)    \n",
    "    #\n",
    "    for col_name in target_col_names:\n",
    "        if flag_print:\n",
    "            print(\"-----------------------------------------\")\n",
    "            print(\"< \", col_name, \" >\")\n",
    "        #\n",
    "        try:\n",
    "            unique_value = df[col_name].dropna().unique()\n",
    "            current_type = str(type(unique_value[0]))\n",
    "            #\n",
    "            if flag_print:\n",
    "                print(\"    current_type = \", current_type)\n",
    "                print(\"    unique_value = \", unique_value )\n",
    "            #\n",
    "            if len(unique_value) == 2:\n",
    "                if current_type == type_str:\n",
    "                    two_level_str.append(col_name)\n",
    "                    if flag_print:\n",
    "                        print(\"    two_level_str\")\n",
    "                else:\n",
    "                    two_level_num.append(col_name)\n",
    "                    if flag_print:\n",
    "                        print(\"    two_level_num\")\n",
    "                #\n",
    "            else:\n",
    "                if current_type == type_str:\n",
    "                    mul_level_str.append(col_name)\n",
    "                    if flag_print:\n",
    "                        print(\"    mul_level_str\")\n",
    "                        \n",
    "                else:\n",
    "                    mul_level_num.append(col_name)\n",
    "                    if flag_print:\n",
    "                        print(\"    mul_level_num\")\n",
    "                    \n",
    "        except:\n",
    "            if flag_print:\n",
    "                print(\"    ---> There is no data. This is already dropped.\")\n",
    "    #\n",
    "    return two_level_num, two_level_str, mul_level_num, mul_level_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_multi_category(feat_info, df):\n",
    "    features_categorical    = feat_info[feat_info[\"type\"] == \"categorical\"].attribute\n",
    "    features_multi_category = []\n",
    "    #\n",
    "    for feature in features_categorical:\n",
    "        if feature in df.columns:\n",
    "            if df[feature].nunique() > 2:\n",
    "                features_multi_category.append(feature)\n",
    "    #\n",
    "    return features_multi_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caee1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_two_num(df, col_names):\n",
    "    if len(col_names) > 0:\n",
    "        for col_name in col_names:\n",
    "            unique_values = df[col_name].unique()\n",
    "            df[col_name].replace(unique_values[0], 200, inplace=True)\n",
    "            df[col_name].replace(unique_values[1], 201, inplace=True)\n",
    "            #\n",
    "            df[col_name].replace(200, 0, inplace=True)\n",
    "            df[col_name].replace(201, 1, inplace=True)\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_two_str(df, col_names):\n",
    "    if len(col_names) > 0:\n",
    "        for col_name in col_names:\n",
    "            unique_values = df[col_name].unique()\n",
    "            df[col_name].replace(unique_values[0], 0, inplace=True)\n",
    "            df[col_name].replace(unique_values[1], 1, inplace=True)\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_mul_level(df, col_names):\n",
    "    if len(col_names) > 0:\n",
    "        df = pd.get_dummies(df, columns=col_names)\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9281164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(feat_info, df, categorical_col_names, flag_print=False):\n",
    "    # get feature names to encode\n",
    "    two_level_num, two_level_str, mul_level_num, mul_level_str = \\\n",
    "        get_categorical_featrues_to_encode(feat_info, df, categorical_col_names, False)\n",
    "    \n",
    "    if flag_print:\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"two_level_num = \", two_level_num)\n",
    "        print(\"two_level_str = \", two_level_str)\n",
    "        print(\"mul_level_num = \", mul_level_num)\n",
    "        print(\"mul_level_str = \", mul_level_str)\n",
    "        print(\"--------------------------------\")    \n",
    "    \n",
    "    #- Policy-1. two level numerical data => 0 or 1\n",
    "    df = encode_categorical_two_num(df, two_level_num)\n",
    "    print(\"    encode_categorical_two_num: shape = \", df.shape)\n",
    "\n",
    "    #- Policy-2. two level character data => 0 or 1\n",
    "    df = encode_categorical_two_str(df, two_level_str)\n",
    "    print(\"    encode_categorical_two_str: shape = \", df.shape)\n",
    "\n",
    "    #- Policy-3. multi level numerical data => one-hot-encoding\n",
    "    df = encode_categorical_mul_level(df, mul_level_num)\n",
    "    print(\"    encode_categorical_mul_level: shape = \", df.shape)\n",
    "    \n",
    "    #- Policy-4. multi level character data => one-hot-encoding\n",
    "    df = encode_categorical_mul_level(df, mul_level_str)\n",
    "    print(\"    encode_categorical_mul_level: shape = \", df.shape)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b890c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(azdias_wo_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43809d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_wo_nan = encode_categorical(feat_info, azdias_wo_nan, categorical_features_now);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906d417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(azdias_wo_nan.shape)\n",
    "display(azdias_wo_nan.head(10))\n",
    "display(azdias_wo_nan.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df89b9b",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_5_2\"></a>\n",
    "### [2-5-2. Encoding : `mixed`](#mokuji_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559786f",
   "metadata": {},
   "source": [
    "### The encoding function : PRAEGENDE_JUGENDJAHRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRAEGENDE_JUGENDJAHRE-1\n",
    "def encode_generations(input_value):\n",
    "    encode_map    = {}\n",
    "    encode_map[0] = [1,2]   # 40s\n",
    "    encode_map[1] = [3,4]   # 50s\n",
    "    encode_map[2] = [5,6,7] # 60s\n",
    "    encode_map[3] = [8,9]   # 70s\n",
    "    encode_map[4] = [10,11,12,13] # 80s\n",
    "    encode_map[5] = [14,15] # 90s\n",
    "    #\n",
    "    try:\n",
    "        for new_value, old_values in encode_map.items():\n",
    "            if input_value in old_values:\n",
    "                return new_value\n",
    "    # When input value is Nan\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "#-----------------------------------------------#\n",
    "# PRAEGENDE_JUGENDJAHRE-2\n",
    "def encode_movement(input_value):\n",
    "    value_of_mainstrem = [1, 3, 5, 8, 10, 12, 14]\n",
    "    #\n",
    "    try:\n",
    "        if input_value in value_of_mainstrem:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    # When input value is Nan\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88424a",
   "metadata": {},
   "source": [
    "### The encoding function : CAMEO_INTL_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------#\n",
    "# CAMEO_INTL_2015-1\n",
    "def encode_wealth(input_value):\n",
    "    if pd.isnull(input_value):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # First digit\n",
    "        return int(str(input_value)[0])\n",
    "    \n",
    "#-----------------------------------------------#\n",
    "# CAMEO_INTL_2015-2\n",
    "def encode_lifestage(input_value):\n",
    "    if pd.isnull(input_value):\n",
    "        return np.nan\n",
    "    else:\n",
    "        # Second digit\n",
    "        return int(str(input_value)[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da69f1",
   "metadata": {},
   "source": [
    "### Drop Features : Mixed Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9890d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library.my_common_tool import list_intersection\n",
    "#-----------------------------------------------#\n",
    "# Mixed Others\n",
    "def get_col_names_to_drop_for_mixed_others(df, feat_info):\n",
    "    features_now       = list(df.columns)\n",
    "    features_mixed_all = list(feat_info[feat_info[\"type\"] == \"mixed\"].attribute)\n",
    "    features_special   = [\"PRAEGENDE_JUGENDJAHRE\", \"CAMEO_INTL_2015\"]\n",
    "    features_mixed_other = set(features_mixed_all) ^ set(features_special)\n",
    "    features_mixed_other = set(features_mixed_other) & set(features_now)\n",
    "    print(\"Drop Features of Mixed = \", features_mixed_other)\n",
    "    df = df.drop(features_mixed_other, axis=1)\n",
    "    return list(features_mixed_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28b237",
   "metadata": {},
   "source": [
    "### The encoding function : For Mixed All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90377daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_mixed(df, feat_info):\n",
    "    # Mixed-1 : \"PRAEGENDE_JUGENDJAHRE\" \n",
    "    if \"PRAEGENDE_JUGENDJAHRE\" in df.columns:\n",
    "        df[\"PRAEGENDE_JUGENDJAHRE_GEN\"] = df[\"PRAEGENDE_JUGENDJAHRE\"].apply(encode_generations)\n",
    "        df[\"PRAEGENDE_JUGENDJAHRE_MOV\"] = df[\"PRAEGENDE_JUGENDJAHRE\"].apply(encode_movement)\n",
    "        df = df.drop([\"PRAEGENDE_JUGENDJAHRE\"], axis=1)\n",
    "    \n",
    "    print(\"    Mixed-1 : PRAEGENDE_JUGENDJAHRE: shape = \", df.shape)\n",
    "    \n",
    "    # Mixed-2 : \"CAMEO_INTL_2015\"\n",
    "    if \"CAMEO_INTL_2015\" in df.columns:\n",
    "        df[\"CAMEO_INTL_2015_WEAL\"] = df[\"CAMEO_INTL_2015\"].apply(encode_wealth)\n",
    "        df[\"CAMEO_INTL_2015_LIFE\"] = df[\"CAMEO_INTL_2015\"].apply(encode_lifestage)\n",
    "        df = df.drop([\"CAMEO_INTL_2015\"], axis=1)    \n",
    "    \n",
    "    print(\"    Mixed-2 : CAMEO_INTL_2015: shape = \", df.shape)\n",
    "\n",
    "    return df\n",
    "#-----------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_wo_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8209a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_wo_nan = encode_mixed(azdias_wo_nan, feat_info)\n",
    "azdias_wo_nan[[\"PRAEGENDE_JUGENDJAHRE_GEN\", \"PRAEGENDE_JUGENDJAHRE_MOV\", \"CAMEO_INTL_2015_WEAL\", \"CAMEO_INTL_2015_LIFE\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_wo_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab93d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_wo_nan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e9f29",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_6\"></a>\n",
    "### [2-6. Scaling](#mokuji_0)\n",
    "\n",
    "Before I apply dimensionality reduction techniques to the data, I need to perform feature scaling so that the principal component vectors are not influenced by the natural differences in scale for features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "scaler_out = std_scaler.fit_transform(azdias_wo_nan)\n",
    "azdias_scaled = pd.DataFrame(scaler_out)\n",
    "azdias_scaled.columns = azdias_wo_nan.columns\n",
    "azdias_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Summary : azdias_scaled\"\n",
    "fig = plot_summary(azdias_scaled, title,figsize=(18,4),fontsize=11)\n",
    "plt.figure(fig)\n",
    "plt.ylim((0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fdb026",
   "metadata": {},
   "source": [
    "### The data size is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50968fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_scaled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629be4d",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_2_7\"></a>\n",
    "### [2-7. Cleaning Function](#mokuji_0)\n",
    "\n",
    "I create a cleaning function `clean_data()` and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, feat_info, my_dict, col_names_to_drop):\n",
    "    \"\"\"\n",
    "    Perform feature trimming, re-encoding, and engineering for demographics data\n",
    "    \n",
    "    INPUT:  Demographics DataFrame\n",
    "    OUTPUT: Trimmed and cleaned demographics DataFrame\n",
    "    \"\"\"\n",
    "    # Converts missing_or_unknown to nan.\n",
    "    print(\"Step-1 : conv_miss_to_nan\")\n",
    "    df    = conv_miss_to_nan(feat_info, df)\n",
    "    print(\"    df.shape = \", df.shape)\n",
    "\n",
    "    # Removes columns which have nans over threshold.\n",
    "    print(\"Step-2 : remove_cols_with_nan\")\n",
    "    df = df.drop(col_names_to_drop, axis=1)        \n",
    "    \n",
    "    # Removes rows which have nans over threshold.\n",
    "    print(\"Step-3 : remove_rows_with_nan\")\n",
    "    df, _ = remove_rows_with_nan(df, thresh_percentage=0.1)\n",
    "    print(\"    df.shape = \", df.shape)\n",
    "    \n",
    "    # Encode Categorical data\n",
    "    print(\"Step-4 : encode_categorical\")\n",
    "    categorical_features_all = list(feat_info[feat_info[\"type\"]==\"categorical\"].attribute)\n",
    "    categorical_features_now = set(list(df.columns)) & set(categorical_features_all)\n",
    "    df = encode_categorical(feat_info, df, categorical_features_now);\n",
    "    print(\"    df.shape = \", df.shape)\n",
    "    \n",
    "    # Encode Mixed data\n",
    "    print(\"Step-5 : encode_mixed\")\n",
    "    df = encode_mixed(df, feat_info)\n",
    "    print(\"    df.shape = \", df.shape)\n",
    "    \n",
    "    # Scale\n",
    "    print(\"Step-6 : StandardScaler\")\n",
    "    std_scaler = StandardScaler()\n",
    "    scaler_out = std_scaler.fit_transform(df)\n",
    "    df_scaled = pd.DataFrame(scaler_out)\n",
    "    df_scaled.columns = df.columns\n",
    "    print(\"    df_scaled.shape = \", df_scaled.shape)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16077c83",
   "metadata": {},
   "source": [
    "### Import new data and apply the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_func_check = pd.read_csv(\"./Data/Proj_02/Udacity_AZDIAS_Subset.csv\", sep=\";\")\n",
    "azdias_func_check = clean_data(azdias_func_check, feat_info, my_dict, col_names_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0c03b",
   "metadata": {},
   "source": [
    "### Check the data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c790f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(azdias_scaled.shape)\n",
    "display(azdias_func_check.shape)\n",
    "print(\"------------------------\")\n",
    "display(azdias_scaled.info())\n",
    "display(azdias_func_check.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4b350",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_3\"></a>\n",
    "## [3. Dimensionality Reduction](#mokuji_0)\n",
    "\n",
    "The size of `scaled data` is very large, so I will apply dimensionality reduction techniques.\n",
    "\n",
    "1. I will use sklearn's [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class to apply **Principal Component Anaysis** on the data.\n",
    "    - This finds the vectors of maximal variance in the data. <br>\n",
    "    - At first, I do not set any parameters or set a number of components that is at least half the number of features.    \n",
    "2. I will check out the variance explained by each principal component as well as the cumulative variance explained.\n",
    "    - I will plot the cumulative or sequential values.\n",
    "    - Based on what I find from the plot, I will select a value for the number of transformed features you'll retain for the clustering part of the project.\n",
    "3. Once I have made a choise for the number of components to keep,\n",
    "    - I will re-fit a PCA instance to perform the decided-on transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7948e97",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_3_1\"></a>\n",
    "### [3-1. PCA, First Trial](#mokuji_0)\n",
    "\n",
    "In this section, I create two functions, and perform `PCA`.\n",
    "1. `my_pca` : This function performs PCA.\n",
    "2. `plot_pca_results` : This function displays cumulative and sequential values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from time import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(data, n_components=0):\n",
    "    if n_components == 0:\n",
    "        model_pca = PCA()\n",
    "    else:\n",
    "        model_pca = PCA(n_components)\n",
    "    #\n",
    "    data_reductioned = model_pca.fit_transform(data)\n",
    "    return model_pca, data_reductioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_results(model_pca, annotation_interval=10, fig_size=(16,5), font_size=14):\n",
    "    # get pca object info\n",
    "    val_components = model_pca.explained_variance_ratio_\n",
    "    val_cum_sum    = np.cumsum(val_components) # Total to accumulate sequentially\n",
    "    num_components = len(val_components)\n",
    "    \n",
    "    # Plot\n",
    "    x_value = list(range(num_components))\n",
    "    #\n",
    "    plt.rcParams[\"font.size\"]      = font_size\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    #\n",
    "    plt.bar( x_value, val_components*100);\n",
    "    plt.plot(x_value, val_cum_sum*100, 'ro-');\n",
    "    plt.grid();\n",
    "    plt.xlabel(\"Principal Components\")\n",
    "    plt.ylabel(\"Variance Explained [%]\")\n",
    "    plt.xticks(np.arange(0, num_components, annotation_interval))\n",
    "    plt.title(\"Explained Variance Per Principal Component\")\n",
    "    \n",
    "    # Annotation\n",
    "    flag_upper = 1\n",
    "    for i in range(num_components):\n",
    "        #\n",
    "        if i%annotation_interval == 0:\n",
    "            tmp_str1 = \"{0:.2f}\".format(val_components[i] * 100)\n",
    "            tmp_str2 = \"{0:.0f}\".format(val_cum_sum[i] * 100)\n",
    "            #\n",
    "            tmp_x   = x_value[i] + 0.2\n",
    "            tmp_y2  = val_cum_sum[i] * 100\n",
    "            #\n",
    "            if flag_upper == 1:\n",
    "                tmp_y1 = val_components[0] * 100\n",
    "                flag_upper = 0\n",
    "            else:\n",
    "                tmp_y1 = val_components[0] * 100 / 2\n",
    "                flag_upper = 1\n",
    "            #\n",
    "            plt.annotate(tmp_str1, (tmp_x, tmp_y1), va=\"bottom\", ha=\"center\", fontsize=font_size )\n",
    "            \n",
    "            if i != 0: # prevent overlapping of annotation\n",
    "                plt.annotate(tmp_str2, (tmp_x, tmp_y2), va=\"bottom\", ha=\"center\", fontsize=font_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783f3ca",
   "metadata": {},
   "source": [
    "### Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 0\n",
    "model_pca, data_reductioned = my_pca(azdias_scaled, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f9668",
   "metadata": {},
   "source": [
    "I show the explanation of the results of PCA.\n",
    "\n",
    "### model_pca.components_\n",
    "- row = dimension\n",
    "- col = columns of the original dataframe\n",
    "- val = eigen vector in each dimension\n",
    "\n",
    "### model_pca.explained_variance_\n",
    "- row = dimension\n",
    "- val = eigen value in each dimension\n",
    "\n",
    "### model_pca.explained_variance_ratio_\n",
    "- row = dimension\n",
    "- val = contribution ratio in each dimension\n",
    "\n",
    "---\n",
    "\n",
    "### Check the data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab15998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"azdias_scaled.shape = \", azdias_scaled.shape)\n",
    "print(\"model_pca.components_.shape = \", model_pca.components_.shape)\n",
    "print(\"model_pca.explained_variance_.shape = \", model_pca.explained_variance_.shape)\n",
    "print(\"model_pca.explained_variance_ratio_.shape = \", model_pca.explained_variance_ratio_.shape)\n",
    "#\n",
    "print(\"data_reductioned.shape      = \", data_reductioned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278fc5d",
   "metadata": {},
   "source": [
    "### Plot PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_results(model_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d49df0",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_3_2\"></a>\n",
    "### [3-2. Check Out the PCA Results](#mokuji_0)\n",
    "\n",
    "### Number of Important features.\n",
    "- From the plot above, if I use the `1/3` of data, I can have `about 70%` of the data information.\n",
    "- So, for 1st trial, I will use `60` components for the dimensionality reduction.\n",
    "\n",
    "### Weights of variables\n",
    "- In the cells below, I check out the weight of each variable on the first few components to see if they can be interpreted in some fashion.\n",
    "\n",
    "- As a reminder, each principal component is a unit vector which points in the direction of highest variance.\n",
    "- If the inner product of two weights (or two unit vectors) is positive, increases in one tend to be associated with increases in the other.\n",
    "- To constrast, if the inner product of two weights is negative, increases in one tend to be associated with decreases in the other.\n",
    "\n",
    "### How to check out the PCA results\n",
    "- I will map each weight to their corresponding feature name, then sort the features according to weight.\n",
    "- I use the data dictionary `my_dict` to help me understand (1)most prominent features, (2)their relationships, and (3)what a positive or negative value on the principal component might indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a973308",
   "metadata": {},
   "source": [
    "### I create a function which translates a PCA model to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_pca_to_df(model_pca, feature_names):\n",
    "    n_components = len(model_pca.components_)\n",
    "    index_name   = [\"PCA_Dim_Id_{}\".format(i) for i in range(n_components)]\n",
    "    #\n",
    "    data_for_df_1 = np.round(model_pca.components_, 4)\n",
    "    data_for_df_2 = model_pca.explained_variance_ratio_.reshape(n_components,1)\n",
    "    #\n",
    "    df_1       = pd.DataFrame(data_for_df_1, columns=feature_names)\n",
    "    df_1.index = index_name\n",
    "    #\n",
    "    df_2       = pd.DataFrame(data_for_df_2, columns=[\"Explained_Variance\"])\n",
    "    df_2.index = index_name\n",
    "    #\n",
    "    return pd.concat([df_2, df_1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = trans_pca_to_df(model_pca, azdias_scaled.columns)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff353a",
   "metadata": {},
   "source": [
    "### I create functions to get information from `my_dict` and the results of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanation_from_my_dict(my_dict, feature_name):\n",
    "    ser_target = my_dict[my_dict[\"attribute\"] == feature_name]\n",
    "    \n",
    "    # Error treatment for the one-hot encoded features whose names are modified\n",
    "    if len(ser_target.id_1) == 0:\n",
    "        pos_last = feature_name.rfind(\"_\")\n",
    "        feature_name_original = feature_name[:pos_last]\n",
    "        ser_target = my_dict[my_dict[\"attribute\"] == feature_name_original]\n",
    "    #\n",
    "    tmp_str = \"{}-{} : {} : {}\".format(ser_target.id_1.values, ser_target.id_2.values, feature_name, ser_target.explanation.values)\n",
    "    tmp_str = tmp_str.replace(\"[\",\"\")\n",
    "    tmp_str = tmp_str.replace(\"]\",\"\")\n",
    "    return tmp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_explanation_from_my_dict(my_dict, \"AGER_TYP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_res(df_pca, my_dict, dimension_id, num_features_to_print=10):\n",
    "    \"\"\"\n",
    "    dimension_id = row id of df_pca to print\n",
    "    num_features_to_print = column num of df_pca to print\n",
    "    \"\"\"\n",
    "    ser_target   = df_pca.iloc[dimension_id, :].copy()\n",
    "    ser_target.sort_values(ascending=False, inplace=True)\n",
    "    n_components = ser_target.shape[0]\n",
    "    #\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    #\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Dimension_{}: Positive Top-{}\".format(dimension_id, num_features_to_print))\n",
    "    print(\"[ranking/ contribution ratio in each dimension/ name/ explanation ]\")\n",
    "    print(\"----------------------------\")\n",
    "    #\n",
    "    for i in range(num_features_to_print):\n",
    "        id = i\n",
    "        tmp_ranking = id + 1\n",
    "        tmp_name    = ser_target.index[id]\n",
    "        tmp_value   = np.round(ser_target.values[id], 3)\n",
    "        tmp_explanation = get_explanation_from_my_dict(my_dict, tmp_name)\n",
    "        #\n",
    "        tmp_list = [tmp_ranking, tmp_value, tmp_name, tmp_explanation]\n",
    "        print(tmp_list)\n",
    "    #\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Dimension_{}: Negative Top-{}\".format(dimension_id, num_features_to_print))\n",
    "    print(\"[ranking/ contribution ratio in each dimension/ name/ explanation ]\")\n",
    "    print(\"----------------------------\")\n",
    "    #\n",
    "    for i in range(num_features_to_print):\n",
    "        id = (n_components-1) - i\n",
    "        tmp_ranking = id + 1\n",
    "        tmp_name    = ser_target.index[id]\n",
    "        tmp_value   = np.round(ser_target.values[id], 3)\n",
    "        tmp_explanation = get_explanation_from_my_dict(my_dict, tmp_name)\n",
    "        #\n",
    "        tmp_list = [tmp_ranking, tmp_value, tmp_name, tmp_explanation]\n",
    "        print(tmp_list)\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04e218",
   "metadata": {},
   "source": [
    "### I show each weight to their corresponding feature name : Dimension ID=0\n",
    "\n",
    "- The positive vectors have relation to the amount of money which people have.\n",
    "- The negative vectors have relation to how to use money for movement pattern of houses and types of houses to live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pca_res(df_pca, my_dict, dimension_id=0, num_features_to_print=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c438643",
   "metadata": {},
   "source": [
    "### I show each weight to their corresponding feature name : Dimension ID=1\n",
    "\n",
    "- The positive vectors have relation to the thinking and beliefs about money.\n",
    "- The negative vectors have relation to the amount of money which people have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064790c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pca_res(df_pca, my_dict, dimension_id=1, num_features_to_print=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e4220",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_3_3\"></a>\n",
    "### [3-3. PCA, Re-Fit](#mokuji_0)\n",
    "\n",
    "I use top 60 components of PCA to create a PCA model `pca60`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca5,  res_pca5  = my_pca(azdias_scaled, n_components=5)\n",
    "model_pca60, res_pca60 = my_pca(azdias_scaled, n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"res_pca60.shape = \", res_pca60.shape)\n",
    "print(\"type(res_pca60) = \", type(res_pca60))\n",
    "print(\"---\")\n",
    "print(\"res_pca60[:2,:] = \", res_pca60[:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481403ee",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_4\"></a>\n",
    "## [4. Clusterling](#mokuji_0)\n",
    "\n",
    "I've assessed and cleaned the demographics, then scaled and transformed them.\n",
    "Now it's time to see how the data clusters in the principal components space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0174d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import k-means library\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf162436",
   "metadata": {},
   "source": [
    "### To decide the number of clusters, I will perform a parameter study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_model_and_score(pca_result, n_clusters):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    model.fit(pca_result)\n",
    "    return np.abs(model.score(pca_result)), model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8e53c",
   "metadata": {},
   "source": [
    "### To perform a parameter study, I create a toy dataset whose size is reduced to 25% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num_all   = res_pca60.shape[0]\n",
    "row_num_25per = int(row_num_all * 0.25)\n",
    "row_random    = np.random.choice(row_num_all, row_num_25per)\n",
    "res_pca60_sample = res_pca60[row_random, :]\n",
    "#\n",
    "print(\"res_pca60.shape        = \", res_pca60.shape)\n",
    "print(\"res_pca60_sample.shape = \", res_pca60_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c145fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_kmeans = []\n",
    "models_kmeans = []\n",
    "cpu_times = []\n",
    "clusters  = [2,5,10,15,20,25,30,40,60,100,200]\n",
    "#\n",
    "for cluster in clusters:\n",
    "    time_s = time()\n",
    "    print(\"Cluster = \", cluster)\n",
    "    tmp_score, tmp_model = get_kmeans_model_and_score(res_pca60_sample, cluster)\n",
    "    time_e = time()\n",
    "    tmp_time = time_e - time_s\n",
    "    #\n",
    "    losses_kmeans.append(tmp_score)\n",
    "    models_kmeans.append(tmp_model)\n",
    "    cpu_times.append(tmp_time)\n",
    "    print(\"    cpu_time = {:.3f}[s]\".format(tmp_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,4)\n",
    "#\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(clusters, losses_kmeans,'ro--' );\n",
    "plt.xlabel('number of clusters')\n",
    "plt.title('Loss')\n",
    "plt.grid()\n",
    "#\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(clusters, cpu_times,'ro--' );\n",
    "plt.xlabel('number of clusters')\n",
    "plt.title('CPU_TIME [s]')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90d084",
   "metadata": {},
   "source": [
    "### From the idea of the [K-Means elbow method ](https://predictivehacks.com/k-means-elbow-method-code-for-python/), I select `40` as a number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b36888",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_s = time()\n",
    "selected_n_clusters = 40\n",
    "loss_kmeans, model_kmeans = get_kmeans_model_and_score(res_pca60, selected_n_clusters)\n",
    "time_e = time()\n",
    "print(\"cpu time[s] = \", (time_e - time_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b01ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans = model_kmeans.predict(res_pca60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc128efa",
   "metadata": {},
   "source": [
    "### `pred_kmeans` is the cluster index of each row (each person).\n",
    "- All of the person is devided to the 40 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de973812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"res_pca60.shape   = \", res_pca60.shape)\n",
    "print(\"pred_kmeans.shape = \", pred_kmeans.shape)\n",
    "print(\"pred_kmeans       = \", pred_kmeans)\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = (12,4);\n",
    "plt.hist(pred_kmeans, bins=40, ec=\"black\");\n",
    "plt.xlabel(\"Kmeans cluster id\");\n",
    "plt.title(\"azdias : number of persons in each cluster\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571636e0",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_4_2\"></a>\n",
    "### [4-2. Apply All Steps to the Customer Data](#mokuji_0)\n",
    "\n",
    "Now that I have clusters and cluster centers for the general population, it's time to see how the customer data maps on to those clusters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e79789",
   "metadata": {},
   "source": [
    "### Load the customer datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef301b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv(\"Data/Proj_02/Udacity_CUSTOMERS_Subset.csv\", sep=\";\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc3dc4",
   "metadata": {},
   "source": [
    "### Clean the customer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eeb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = clean_data(customers, feat_info, my_dict, col_names_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d3e27",
   "metadata": {},
   "source": [
    "### Apply PCA to the `customers` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e170b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pca_customers = model_pca60.transform(customers)\n",
    "pred_kmeans_customers = model_kmeans.predict(res_pca_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830a55e",
   "metadata": {},
   "source": [
    "<a id=\"mokuji_4_3\"></a>\n",
    "### [4-3. Compare Customer Data to Demographics Data](#mokuji_0)\n",
    "\n",
    "At this point, I have clustered the dataset based on demographics of the general population of Germany(= `azdias`), and seen how the customer data for a mail-order sales company maps onto those demographic clusters.\n",
    "\n",
    "In this final substep, I will compare the two cluster distributions to see where the strongest customer base for the company is.\n",
    "\n",
    "I will consider the proportion of persons in each cluster for the general population, and the proportions for the customers.\n",
    "\n",
    "If we think the company's customer base to be universal, then the cluster assignment proportions should be fairly similar between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd24a4",
   "metadata": {},
   "source": [
    "### Compair the number of persons in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f70326",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,4);\n",
    "plt.hist([pred_kmeans, pred_kmeans_customers], bins=40, ec=\"black\", label=[\"azdias\", \"customers\"]);\n",
    "plt.xlabel(\"Kmeans cluster id\");\n",
    "plt.title(\"number of persons in each cluster\");\n",
    "plt.legend(loc = \"upper center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9a738",
   "metadata": {},
   "source": [
    "### Compair the number of persons in each cluster using `Normalization`\n",
    "\n",
    "To investigate the similarity of the cluster assignment proportion between the two, I will show two graphs.\n",
    "1. normalized number of persons\n",
    "2. ratio of number of persons of the two dataset\n",
    "\n",
    "From the graph below, it is found that there are differences of the cluster assignment proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_hist(data, bins):\n",
    "    sum = np.sum(data)\n",
    "    counts, _ = np.histogram(data, bins=bins)\n",
    "    return (counts/sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b70e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans_azdias_norm    = get_normalized_hist(pred_kmeans, 40)\n",
    "pred_kmeans_customers_norm = get_normalized_hist(pred_kmeans_customers, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_x1 = np.arange(0,40)\n",
    "bar_x2 = bar_x1 + 0.3\n",
    "plt.rcParams['figure.figsize'] = (12,4);\n",
    "plt.bar(bar_x1, pred_kmeans_azdias_norm,    color=\"blue\",   width=0.3, label=\"azdias\", align=\"center\")\n",
    "plt.bar(bar_x2, pred_kmeans_customers_norm, color=\"orange\", width=0.3, label=\"customers\", align=\"center\")\n",
    "plt.xlabel(\"Kmeans cluster id\");\n",
    "plt.title(\"ratio of persons in each cluster with normalization\");\n",
    "plt.legend(loc = \"upper center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcd1e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar_x1 = np.arange(0,40)\n",
    "bar_x2 = bar_x1 + 0.3\n",
    "pred_kmeans_comp = pred_kmeans_customers_norm / pred_kmeans_azdias_norm\n",
    "max_cluster_id = np.argmax(pred_kmeans_comp)\n",
    "min_cluster_id = np.argmin(pred_kmeans_comp)\n",
    "plt.rcParams['figure.figsize'] = (12,4);\n",
    "plt.bar(bar_x1, pred_kmeans_comp,    color=\"blue\",   width=0.3, label=\"customers / azdias\", align=\"center\")\n",
    "plt.xlabel(\"Kmeans cluster id\");\n",
    "plt.title(\"ratio = customers / adzias\");\n",
    "plt.legend(loc = \"upper center\")\n",
    "plt.grid()\n",
    "#\n",
    "print(\"Max Ratio Cluster ID = {} / ratio = {:.3f}\".format(max_cluster_id, pred_kmeans_comp[max_cluster_id]))\n",
    "print(\"Min Ratio Cluster ID = {} / ratio = {:.3f}\".format(min_cluster_id, pred_kmeans_comp[min_cluster_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e5d56",
   "metadata": {},
   "source": [
    "### What kinds of people are part of a cluster that is overrepresented in the customer data compared to the general population(= `azdias`) ?\n",
    "\n",
    "1. Selecte a cluster id (= `target_cluster_id`) which is overrepresented in `customer` compared to `azdias`.\n",
    "  - Select one from 40 clusters of the KMeans result.\n",
    "  \n",
    "2. Get a cluster center (= `pca_cluster_center`) which has information about principal components.\n",
    "\n",
    "3. Transform principal components to the scaled original value.(`pca_cluster_center` --> `df_reversed`)<br>\n",
    "  - use `pca.inverse_transform()`\n",
    "\n",
    "4. Investigate top 2 principal components Id (= `top_pca_ids`) of `pca_cluster_center`.\n",
    "\n",
    "5. For top 2 components(`top_pca_ids`), get each features weight( = importance )<br>\n",
    "\n",
    "  5-1. get each principal component\n",
    "    - the row size of principal components = 1\n",
    "    - the col size of principal components = number of features\n",
    "\n",
    "  5-2. `weight` = `principal components` * `df_reversed`\n",
    "\n",
    "6. Using positive and negative top 5 `weight`, I will investigate which features are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_kmeans.cluster_centers_.shape)\n",
    "print(model_kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322f480",
   "metadata": {},
   "source": [
    "### 4-3-1. Selecte a cluster id which is overrepresented in customer compared to azdias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13088a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cluster_id  = max_cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d626e",
   "metadata": {},
   "source": [
    "### 4-3-2. Get a cluster center which has information about principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cluster_center = model_kmeans.cluster_centers_[target_cluster_id]\n",
    "ser_pca_cluster_center = pd.Series(pca_cluster_center)\n",
    "ser_pca_cluster_center.sort_values(ascending=False, inplace=True)\n",
    "#\n",
    "print(\"pca_cluster_center.shape = \", pca_cluster_center.shape)\n",
    "display(ser_pca_cluster_center.head())\n",
    "#\n",
    "top_pca_ids = list(ser_pca_cluster_center.index[:2])\n",
    "print(\"top_pca_ids = \", top_pca_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db904893",
   "metadata": {},
   "source": [
    "### 4-3-3. Transform principal components to the scaled original value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df204e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_reversed = model_pca60.inverse_transform(ser_pca_cluster_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0319813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reversed = pd.DataFrame(ser_reversed, columns=[\"cluster_27\"]).round()\n",
    "df_reversed.index = customers.columns\n",
    "display(df_reversed.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ba202",
   "metadata": {},
   "source": [
    "### Get `numpy.ndarray` from `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82705835",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_reversed = df_reversed.iloc[:,0].values\n",
    "print(values_reversed.shape)\n",
    "print(type(values_reversed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b99338c",
   "metadata": {},
   "source": [
    "### 4-3-5. For top 2 components, get each features weight\n",
    "- I select\n",
    "  - 1st principal component id = 36\n",
    "  - 2nd principal component id = 5\n",
    "  \n",
    "- To calculate the weight of each feature, I use the `df_pca` which is created in the above cell and has the principal components values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca2 = df_pca.drop(axis=1,columns=[\"Explained_Variance\"], inplace=False)\n",
    "display(df_pca2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488216d7",
   "metadata": {},
   "source": [
    "### pick up principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c49df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_comp_36 = df_pca2.iloc[36, :].values\n",
    "pca_comp_5  = df_pca2.iloc[5, :].values\n",
    "\n",
    "print(pca_comp_36.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0443a",
   "metadata": {},
   "source": [
    "### multiply reversed values and principal components and get weights of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83140fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_pca_36 = pd.Series(values_reversed * pca_comp_36)\n",
    "ser_pca_5  = pd.Series(values_reversed * pca_comp_5)\n",
    "#\n",
    "ser_pca_36.index = customers.columns\n",
    "ser_pca_5.index = customers.columns\n",
    "#\n",
    "ser_pca_36.sort_values(ascending=False, inplace=True)\n",
    "ser_pca_5.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weight_and_explanation(ser, my_dict, num_disp=5):\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"[weight] : [explanation]\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    col_names = ser.index\n",
    "    num_row   = len(ser.index)\n",
    "    #\n",
    "    print(\"----------------- Top 5 -----------------\")\n",
    "    #\n",
    "    for i in range(num_disp):\n",
    "        col_name = col_names[i]\n",
    "        weight = ser.iloc[i]\n",
    "        exp = get_explanation_from_my_dict(my_dict, col_name)\n",
    "        print(\"[{:.3f}] : [{}]\".format(weight, exp))\n",
    "    #\n",
    "    print(\" \")\n",
    "    print(\"----------------- Tail 5 -----------------\")\n",
    "    #\n",
    "    for i in range(num_disp):\n",
    "        j = num_row - i - 1\n",
    "        col_name = col_names[j]\n",
    "        weight = ser.iloc[j]\n",
    "        exp = get_explanation_from_my_dict(my_dict, col_name)\n",
    "        print(\"[{:.3f}] : [{}]\".format(weight, exp))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca976ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weight_and_explanation(ser_pca_36, my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ffd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weight_and_explanation(ser_pca_5, my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be4d2e",
   "metadata": {},
   "source": [
    "### Observation about the results above\n",
    "\n",
    "Cluster 27 is one of the overrepresented clusters in the customer dataset compared to the general population. Looking at most 2 influencing components and their weights, the cluster represents social status and house types which they live in. \n",
    "\n",
    "From the `Data_Dictionary.md` and the results above, the mail-order company is recommended to focus on the next people,\n",
    "- title holder-households\n",
    "- new houseowners\n",
    "- independents\n",
    "- City Nobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb20a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
